{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"--gpu_ids\", \"0\", \"--shuffle_topo\", \"--run_test_freq\", \"10\", \"--ncf\", \"16\", \"16\", \"16\", \"16\", \"16\", \"16\", \"--niter_decay\", \"10\", \"--niter\", \"150\", \"--resblocks\", \"3\", \"--lr\", \"0.001\", \"--drop_relu\", \"--num_threads\", \"0\", \"--selection_module\", \"--reweight_loss\", \"--dataset_mode\", \"intseg\", \"--arch\", \"meshcnn\", \"--batch_size\", \"8\", \"--which_epoch\", \"best\", \"--extrinsic_condition_placement\", \"pre\", \"--extrinsic_features\", \"onehot\", \"hks\", \"--edgefeatures\", \"dihedrals\", \"--distortion_metric\", \"arap\", \"--epoch_count\", \"1\", \"--resconv\", \"--leakyrelu\", \"--layernorm\", \"--cachefolder\", \"cache\", \"--anchorcachefolder\", \"anchorcache\", \"--delayed_distortion_epochs\", \"0\", \"--step2paramloss\", \"--export_save_path\", \"outputs\", \"--checkpoints_dir\", \"outputs\", \"--dropout\", \"--anchor_loss\", \"--max_sample_size\", \"200\", \"--max_dataset_size\", \"5000\", \"--load_pretrain\", \"--which_epoch\", \"latest\", \"--extrinsic_condition_placement\", \"pre\", \"--extrinsic_features\", \"onehot\", \"hks\", \"--edgefeatures\", \"dihedrals\", \"--network_load_path\", \"./outputs/pretrain_aug\", \"--name\", \"9_dcount_v2_dschedTrue_mixTrue_augFalse\", \"--cachefolder\", \"ftcache9\", \"--anchorcachefolder\", \"ftanchorcache9\", \"--export_save_path\", \"outputs/distortion_finetune\", \"--checkpoints_dir\", \"outputs/distortion_finetune\", \"--distortion_loss\", \"count_v2\", \"--deltaschedule\", \"--delta\", \"1000\", \"--min_delta\", \"1\", \"--dataroot\", \"datasets/mixed\", \"--test_dir\", \"datasets/mixed/test\", \"--mixedtraining\", \"--supervised\"]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "basestr = \"--gpu_ids 0 --shuffle_topo --run_test_freq 10 --ncf 16 16 16 16 16 16 --niter_decay 10 --niter 150 --resblocks 3 --lr 0.001 --drop_relu --num_threads 0 --selection_module --reweight_loss --dataset_mode intseg --arch meshcnn --batch_size 8  --which_epoch best --extrinsic_condition_placement pre --extrinsic_features onehot hks --edgefeatures dihedrals --distortion_metric arap --epoch_count 1 --resconv --leakyrelu --layernorm --cachefolder cache --anchorcachefolder anchorcache --delayed_distortion_epochs 0 --step2paramloss --export_save_path outputs --checkpoints_dir outputs --dropout --anchor_loss --max_sample_size 200 --max_dataset_size 5000 --load_pretrain --which_epoch latest --extrinsic_condition_placement pre --extrinsic_features onehot hks --edgefeatures dihedrals --network_load_path ./outputs/pretrain_aug --name 9_dcount_v2_dschedTrue_mixTrue_augFalse --cachefolder ftcache9 --anchorcachefolder ftanchorcache9 --export_save_path outputs/distortion_finetune --checkpoints_dir outputs/distortion_finetune --distortion_loss count_v2 --deltaschedule --delta 1000 --min_delta 1 --dataroot datasets/mixed --test_dir datasets/mixed/test --mixedtraining --supervised\"\n",
    "print(json.dumps(basestr.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from models.layers.meshing import Mesh \n",
    "from models.layers.meshing.io import PolygonSoup\n",
    "import dill as pickle \n",
    "import numpy as np \n",
    "from pathlib import Path \n",
    "from util.util import clear_directory\n",
    "import shutil\n",
    "from models.layers.meshing.analysis import computeFaceAreas, computeDihedrals, computeHKS, computeVertexNormals, computeFaceNormals\n",
    "\n",
    "# Construct mixed dataset \n",
    "synthratio = 0.25 \n",
    "thingiratio = 0.75\n",
    "synthdataset = \"./datasets/synthetic_dataset\"\n",
    "thingidataset = \"./datasets/thingi10k\"\n",
    "max_dataset_size = 5000\n",
    "\n",
    "mixeddataset = \"./datasets/mixed\"\n",
    "\n",
    "# Count total number of samples through labels\n",
    "synthmeshes = {'train': [], 'test': []}\n",
    "thingimeshes = {'train': [], 'test': []}\n",
    "for folder in ['train', 'test']:\n",
    "    count = 0 \n",
    "    for labelfile in os.listdir(os.path.join(synthdataset, folder, \"labels\")):\n",
    "        if labelfile.endswith(\".npy\"):\n",
    "            count += 1\n",
    "    print(f\"Synth {folder}: {count}\")\n",
    "    \n",
    "    for objfile in os.listdir(os.path.join(synthdataset, folder)):\n",
    "        if objfile.endswith(\".obj\"):\n",
    "            synthmeshes[folder].append(objfile)\n",
    "    \n",
    "    count = 0\n",
    "    for objfile in os.listdir(os.path.join(thingidataset, folder)):\n",
    "        if objfile.endswith(\".obj\"):\n",
    "            thingimeshes[folder].append(objfile)\n",
    "            \n",
    "            with open(os.path.join(thingidataset, folder, \"anchors\", objfile.replace(\".obj\", \"\") + \".pkl\"), 'rb') as f:\n",
    "                anchors = pickle.load(f)\n",
    "            count += len(anchors)\n",
    "    print(f\"Thingi {folder}: {count}\")\n",
    "\n",
    "# TODO: filter thingi meshes by size + hks calculation \n",
    "if os.path.exists(mixeddataset):\n",
    "    clear_directory(mixeddataset)\n",
    "    \n",
    "for folder in ['train', 'test']:\n",
    "    datadir = os.path.join(mixeddataset, folder)\n",
    "    anchordir = os.path.join(mixeddataset, folder, 'anchors')\n",
    "    Path(anchordir).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    count = 0\n",
    "    while count < max_dataset_size:\n",
    "        if np.random.uniform(0,1) <= synthratio: \n",
    "            labeldir = os.path.join(mixeddataset, folder, 'labels')\n",
    "            Path(labeldir).mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "            # Randomly choose a synthetic mesh to use\n",
    "            synthmesh = np.random.choice(synthmeshes[folder])\n",
    "            meshname = synthmesh.replace(\".obj\", \"\")\n",
    "            \n",
    "            # Copy obj, labels, anchors\n",
    "            shutil.copyfile(os.path.join(synthdataset, folder, synthmesh), \n",
    "                            os.path.join(datadir, synthmesh))\n",
    "            shutil.copyfile(os.path.join(synthdataset, folder, \"anchors\", synthmesh.replace(\".obj\", \"\") + \".pkl\"), \n",
    "                            os.path.join(anchordir, synthmesh.replace(\".obj\", \"\") + \".pkl\"))\n",
    "            \n",
    "            with open(os.path.join(synthdataset, folder, \"anchors\", synthmesh.replace(\".obj\", \"\") + \".pkl\"), 'rb') as f:\n",
    "                anchors = pickle.load(f) \n",
    "            \n",
    "            for i in range(len(anchors)):\n",
    "                shutil.copyfile(os.path.join(synthdataset, folder, \"labels\", meshname + f\"_{i}.npy\"), \n",
    "                                os.path.join(labeldir, meshname + f\"_{i}.npy\"))\n",
    "            count += len(anchors)\n",
    "            \n",
    "            synthmeshes[folder].remove(synthmesh)\n",
    "        else:\n",
    "            while True:\n",
    "                # Randomly choose a synthetic mesh to use \n",
    "                thingimesh = np.random.choice(thingimeshes[folder])\n",
    "                meshname = thingimesh.replace(\".obj\", \"\")\n",
    "                \n",
    "                ## Check mesh is good \n",
    "                soup = PolygonSoup.from_obj(os.path.join(thingidataset, folder, thingimesh)) \n",
    "                mesh = Mesh(soup.vertices, soup.indices)\n",
    "\n",
    "                if len(mesh.topology.edges) > 20000:\n",
    "                    print(f\"Skipping too large mesh {thingimesh} ...\")\n",
    "                    continue \n",
    "                \n",
    "                # Check if valid mesh \n",
    "                if mesh.topology.hasNonManifoldEdges():\n",
    "                    print(f\"Skipping nonmanifold mesh {thingimesh} ...\")\n",
    "                    continue\n",
    "                \n",
    "                try: \n",
    "                    mesh.topology.thorough_check()\n",
    "                except Exception as e:\n",
    "                    print(e) \n",
    "                    print(f\"Failed thorough check mesh {thingimesh} ...\")\n",
    "                    continue \n",
    "                \n",
    "                try:\n",
    "                    vs, fs, es = mesh.export_soup() \n",
    "                except Exception as e:\n",
    "                    print(e) \n",
    "                    print(f\"Failed export mesh {thingimesh} ...\")\n",
    "                    continue \n",
    "                \n",
    "                # Try computing HKS and heat geodesic\n",
    "                try:\n",
    "                    from igl import edge_lengths, heat_geodesic\n",
    "                    from util.util import compute_hks\n",
    "                    from util.diffusion_net.geometry import get_operators\n",
    "                    import torch \n",
    "                    \n",
    "                    scales = torch.logspace(-2, 0, 16)\n",
    "                    vertices = torch.from_numpy(mesh.vertices).float()\n",
    "                    faces = torch.from_numpy(np.ascontiguousarray(mesh.faces)).long()\n",
    "\n",
    "                    frames, mass, L, evals, evecs, gradX, gradY = \\\n",
    "                        get_operators(vertices, faces, meshname, op_cache_dir=os.path.join(thingidataset, folder, \"opcache\"), overwrite_cache=True)\n",
    "                    mesh.hks = compute_hks(evals, evecs, scales).detach().numpy()\n",
    "                    \n",
    "                    t = np.mean(edge_lengths(mesh.vertices, mesh.faces))**2\n",
    "                    geodesics = heat_geodesic(mesh.vertices, mesh.faces, t, np.array([0]))\n",
    "                except Exception as e:\n",
    "                    print(e) \n",
    "                    print(f\"Failed HKS/heat geodesic mesh {thingimesh} ...\")\n",
    "                    continue \n",
    "            \n",
    "                # Check connected components \n",
    "                import pymeshlab \n",
    "                ms = pymeshlab.MeshSet()\n",
    "                ms.clear() \n",
    "                ms.load_new_mesh(os.path.join(thingidataset, folder, thingimesh)) \n",
    "                topo = ms.get_topological_measures()\n",
    "                \n",
    "                # Components \n",
    "                components = topo['connected_components_number']\n",
    "                if components > 1: \n",
    "                    print(f\"Multiple components found for {thingimesh}\")\n",
    "                    continue \n",
    "                \n",
    "                break \n",
    "                \n",
    "            # Copy obj, labels, anchors\n",
    "            shutil.copyfile(os.path.join(thingidataset, folder, thingimesh), \n",
    "                            os.path.join(datadir, thingimesh))\n",
    "            shutil.copyfile(os.path.join(thingidataset, folder, \"anchors\", thingimesh.replace(\".obj\", \"\") + \".pkl\"), \n",
    "                            os.path.join(anchordir, thingimesh.replace(\".obj\", \"\") + \".pkl\"))\n",
    "            with open(os.path.join(thingidataset, folder, \"anchors\", thingimesh.replace(\".obj\", \"\") + \".pkl\"), 'rb') as f:\n",
    "                anchors = pickle.load(f) \n",
    "            count += len(anchors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/projects/ranalab/guanzhi/.conda/envs/dawand/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Filter meshes by edge counts \n",
    "from models.layers.meshing import Mesh \n",
    "from models.layers.meshing.io import PolygonSoup\n",
    "import os \n",
    "\n",
    "datadir = \"./datasets/thingi10k\"\n",
    "for folder in ['test', 'train']:\n",
    "    for objfile in os.listdir(os.path.join(datadir, folder)):\n",
    "        if not objfile.endswith(\".obj\"):\n",
    "            continue \n",
    "        soup = PolygonSoup.from_obj(os.path.join(datadir, folder, objfile)) \n",
    "        mesh = Mesh(soup.vertices, soup.indices)\n",
    "        \n",
    "        if len(mesh.topology.edges) > 25000:\n",
    "            print(f\"Removing file {os.path.join(datadir, folder, objfile)} with edges {len(mesh.topology.edges)}...\")\n",
    "            os.remove(os.path.join(datadir, folder, objfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Thingi10k finetuning \n",
    "basestr = \"python train.py --gpu_ids 0 --shuffle_topo --run_test_freq 10 --ncf 16 16 16 16 16 16 \" \\\n",
    "\"--niter_decay 10 --niter 150 --resblocks 3 --lr 0.001 --drop_relu --num_threads 0 \" \\\n",
    "\"--selection_module --reweight_loss --dataset_mode intseg --arch meshcnn --batch_size 8  \" \\\n",
    "\"--which_epoch best --extrinsic_condition_placement pre --extrinsic_features onehot hks --edgefeatures dihedrals \" \\\n",
    "\"--distortion_metric arap --epoch_count 1 --resconv --leakyrelu --layernorm \" \\\n",
    "\"--cachefolder cache --anchorcachefolder anchorcache --delayed_distortion_epochs 0 --step2paramloss --export_save_path outputs \" \\\n",
    "\"--checkpoints_dir outputs --dropout \" \\\n",
    "\"--anchor_loss --max_sample_size 200 --max_dataset_size 5000 \"\n",
    "\n",
    "\n",
    "# Network specific \n",
    "basestr += \"--load_pretrain --which_epoch latest --extrinsic_condition_placement pre --extrinsic_features onehot hks \" \\\n",
    "    \"--edgefeatures dihedrals --network_load_path ./outputs/pretrain_aug \"\n",
    "    \n",
    "shape_file = open(\"./slurm/distortion_finetune\", \"w\")\n",
    "retrieve_shape_file = open(\"./slurm/distortion_finetune_net\", \"w\")\n",
    "render_shape_file = open(\"./slurm/distortion_finetune_render\", \"w\")\n",
    "\n",
    "# We have enough room to run every permutation of the above \n",
    "from itertools import product \n",
    "distortion_losses = ['count', 'count_v2', 'count_l0'] \n",
    "delta_schedule = [True, False] # countv2: 1000, count: min_delta = 0.01, delta = 2, count_l0: min_delta = 0.01, delta = 1\n",
    "mixed = [True, False]\n",
    "augs = [True, False]\n",
    "\n",
    "settings = list(product(distortion_losses, delta_schedule, mixed, augs))\n",
    "outdir = \"distortion_finetune\"\n",
    "\n",
    "i = 0 \n",
    "# Mixed jobs\n",
    "for distortion_loss, delta_sched, mix, aug in settings:\n",
    "    name = f\"{i}_d{distortion_loss}_dsched{delta_sched}_mix{mix}_aug{aug}\"\n",
    "    full_str = basestr + f\"--name {name} --cachefolder ftcache{i} --anchorcachefolder ftanchorcache{i} \"\n",
    "    full_str += f\"--export_save_path outputs/{outdir} --checkpoints_dir outputs/{outdir} --distortion_loss {distortion_loss} \"\n",
    "    \n",
    "    if delta_sched and distortion_loss == \"count\": \n",
    "        full_str += f\"--deltaschedule --delta 2 --min_delta 0.01 \"\n",
    "    if delta_sched and distortion_loss == \"count_v2\": \n",
    "        full_str += f\"--deltaschedule --delta 1000 --min_delta 1 \"\n",
    "    if delta_sched and distortion_loss == \"count_l0\": \n",
    "        full_str += f\"--deltaschedule --delta 1 --min_delta 0.01 \"\n",
    "    \n",
    "    if mix: \n",
    "        full_str += f\"--dataroot datasets/mixed --test_dir datasets/mixed/test --mixedtraining --supervised \"\n",
    "    else:\n",
    "        full_str += f\"--dataroot ./datasets/thingi10k --test_dir ./datasets/thingi10k/test \"\n",
    "        \n",
    "    # Run with augs\n",
    "    if aug:\n",
    "        full_str += \"--num_aug 1 --rotaug --vnormaug --testaug \"\n",
    "        \n",
    "    shape_file.write(full_str + \"\\n\")\n",
    "    retrieve_shape_file.write(f\"python get_net_epoch.py --dir outputs/{outdir} --name {name}\\n\")\n",
    "    render_shape_file.write(f\"python render_preds.py --dir outputs/{outdir} --name {name} --render_uv\\n\")\n",
    "    i += 1\n",
    "            \n",
    "shape_file.close()\n",
    "retrieve_shape_file.close()\n",
    "render_shape_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"--gpu_ids\", \"0\", \"--mode\", \"evaluation\", \"--shuffle_topo\", \"--export_view_freq\", \"4\", \"--run_test_freq\", \"4\", \"--ncf\", \"16\", \"16\", \"16\", \"16\", \"16\", \"16\", \"--niter_decay\", \"10\", \"--niter\", \"150\", \"--resblocks\", \"3\", \"--lr\", \"0.001\", \"--drop_relu\", \"--num_threads\", \"0\", \"--save_latest_freq\", \"10\", \"--save_epoch_freq\", \"2\", \"--selection_module\", \"--reweight_loss\", \"--dataset_mode\", \"intseg\", \"--arch\", \"meshcnn\", \"--resconv\", \"--batch_size\", \"8\", \"--leakyrelu\", \"--layernorm\", \"--which_epoch\", \"best\", \"--extrinsic_condition_placement\", \"pre\", \"--extrinsic_features\", \"onehot\", \"hks\", \"--edgefeatures\", \"dihedrals\", \"--export_preds\", \"--distortion_metric\", \"arap\", \"--cachefolder\", \"cache\", \"--anchorcachefolder\", \"anchorcache\", \"--delayed_distortion_epochs\", \"0\", \"--step2paramloss\", \"--export_save_path\", \"outputs\", \"--checkpoints_dir\", \"outputs\", \"--supervised\", \"--dataroot\", \"./datasets/thingi10k\", \"--test_dir\", \"./datasets/thingi10k/test\", \"--anchor_loss\", \"--max_sample_size\", \"200\", \"--max_dataset_size\", \"5000\", \"--distortion_loss\", \"count_v2\", \"--gcsmoothness\", \"--gcsmoothness_weight\", \"0.001\", \"--num_aug\", \"1\", \"--vnormaug\", \"--testaug\", \"--max_grad\", \"0.1\", \"--name\", \"grad0.1_losscount_v2_augTrue\"]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "cmds = \"--gpu_ids 0 --mode evaluation --shuffle_topo --export_view_freq 4 --run_test_freq 4 --ncf 16 16 16 16 16 16 --niter_decay 10 --niter 150 --resblocks 3 --lr 0.001 --drop_relu --num_threads 0 --save_latest_freq 10 --save_epoch_freq 2 --selection_module --reweight_loss --dataset_mode intseg --arch meshcnn --resconv --batch_size 8 --leakyrelu --layernorm --which_epoch best --extrinsic_condition_placement pre --extrinsic_features onehot hks --edgefeatures dihedrals --export_preds --distortion_metric arap --cachefolder cache --anchorcachefolder anchorcache --delayed_distortion_epochs 0 --step2paramloss --export_save_path outputs --checkpoints_dir outputs --supervised --dataroot ./datasets/thingi10k --test_dir ./datasets/thingi10k/test --anchor_loss --max_sample_size 200 --max_dataset_size 5000  --distortion_loss count_v2 --gcsmoothness --gcsmoothness_weight 0.001 --num_aug 1 --vnormaug --testaug --max_grad 0.1 --name grad0.1_losscount_v2_augTrue\"\n",
    "print(json.dumps(cmds.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different NO pre-training training options \n",
    "import os \n",
    "\n",
    "# TODO: test these values \n",
    "max_grads = [None, 0.1, 1, 5]\n",
    "smoothness = [None, \"default\", \"anneal\"]\n",
    "inits = [None, 'anchor', 'neighbors']\n",
    "augs = [True, False]\n",
    "losses = ['count', 'count_v2']\n",
    "datasets = ['pretrain', 'thingi10k']\n",
    "\n",
    "basestr = \"python train.py --gpu_ids 0 --mode evaluation --shuffle_topo --export_view_freq 4 --run_test_freq 4 --ncf 16 16 16 16 16 16 \" \\\n",
    "\"--niter_decay 10 --niter 150 --resblocks 3 --lr 0.001 --drop_relu --num_threads 0 --save_latest_freq 10 --save_epoch_freq 2 \" \\\n",
    "\"--selection_module --reweight_loss --dataset_mode intseg --arch meshcnn --resconv --batch_size 4 --leakyrelu --layernorm \" \\\n",
    "\"--which_epoch best --extrinsic_condition_placement pre --extrinsic_features onehot hks --edgefeatures dihedrals \" \\\n",
    "\"--export_preds --distortion_metric arap --epoch_count 1 \" \\\n",
    "\"--cachefolder cache --anchorcachefolder anchorcache --delayed_distortion_epochs 0 --step2paramloss --export_save_path outputs \" \\\n",
    "\"--checkpoints_dir outputs --supervised --dataroot ./datasets/thingi10k --test_dir ./datasets/thingi10k/test \" \\\n",
    "\"--anchor_loss --max_sample_size 200 --max_dataset_size 5000 --plot_preds \"\n",
    "\n",
    "f = open(\"slurm/nopretrain_jobs\", 'w')\n",
    "netf = open(\"slurm/nopretrain_jobs_net\", 'w') \n",
    "## First test just the gradient clamping with the different loss types and augs \n",
    "for grad in max_grads:\n",
    "    for loss in losses:\n",
    "        for aug in augs:\n",
    "            expstr = basestr + f\" --distortion_loss {loss} --gcsmoothness --gcsmoothness_weight 0.001\"\n",
    "            if aug:\n",
    "                expstr += \" --num_aug 1 --vnormaug --testaug\"\n",
    "            if grad:\n",
    "                expstr += f\" --max_grad {grad}\"\n",
    "            name = f\"grad{grad}_loss{loss}_aug{aug}\"\n",
    "            expstr += f\" --name {name}\"\n",
    "            netstr =  f\"python get_net_epoch.py --dir outputs --name {name}\"\n",
    "            f.write(expstr + \"\\n\")\n",
    "            netf.write(netstr + \"\\n\")\n",
    "f.close() \n",
    "netf.close()\n",
    "\n",
    "# for dataest in datasets: \n",
    "#     for loss in losses: \n",
    "#         for init in inits: \n",
    "#             for smooth in smoothness: \n",
    "#                 for grad in max_grads:\n",
    "#                     for aug in augs:\n",
    "#                         if aug:\n",
    "#                             expstr = basestr + f\" --num_aug 1 --vnormaug --testaug \" \n",
    "#                         else:\n",
    "#                             expstr = basestr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different pre-training options \n",
    "augs = [True, False]\n",
    "basestr = \"python train.py --gpu_ids 0 --mode evaluation --shuffle_topo --export_preds --export_view_freq 10 --run_test_freq 10 \" \\\n",
    "    \"--ncf 16 16 16 16 16 16 --niter_decay 10 --niter 150 --resblocks 3 --lr 0.001 --drop_relu --num_threads 0 --save_latest_freq 10 \" \\\n",
    "    \"--save_epoch_freq 2 --selection_module --reweight_loss --dataset_mode intseg --arch meshcnn --test_dir ./datasets/synthetic_dataset/test \" \\\n",
    "    \"--dataroot ./datasets/synthetic_dataset --checkpoints_dir outputs --max_sample_size 8000 --supervised --epoch_count 1 \" \\\n",
    "    \"--extrinsic_condition_placement pre --batch_size 15 --name pretrain --cachefolder cache --anchorcachefolder anchorcache --resconv \" \\\n",
    "    \"--batch_size 8 --leakyrelu --layernorm --dropout --extrinsic_features onehot hks --edgefeatures dihedrals --plot_preds \"\n",
    "base_netstr = \"python get_net_epoch.py\"\n",
    "\n",
    "f = open(\"slurm/pretrain_jobs\", 'w')\n",
    "netf = open(\"slurm/pretrain_jobs_net\", 'w') \n",
    "for aug in augs:\n",
    "    if aug:\n",
    "        expstr = basestr + f\" --export_save_path outputs --num_aug 1 --vnormaug --testaug --name pretrain_aug\" \n",
    "        netstr = base_netstr + f\" --dir outputs --name pretrain_aug\"\n",
    "    else:\n",
    "        expstr = basestr + f\" --export_save_path outputs --name pretrain\"\n",
    "        netstr = base_netstr + f\" --dir outputs --name pretrain_aug\"\n",
    "    f.write(expstr + \"\\n\")\n",
    "    netf.write(netstr + \"\\n\")\n",
    "f.close()\n",
    "netf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dawand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
